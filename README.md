# Echo(I) - Learning Hierarchical Influence Propagation in Organizational Response Systems

A deep learning framework for predicting organizational responses to recommendations using game theory, graph neural networks, and natural language processing.

## ğŸ“‹ Project Overview

This repository contains the implementation for the Echo(I) project as part of MIT's 6.S890 course. The project develops a novel framework that integrates **game-theoretic formalization** with **deep learning architectures** to model and predict organizational response dynamics.

### Core Innovation

We formalize organizational response as a **Hierarchical Partially Observable Stochastic Game (H-POSG)** and learn effective representations from text descriptions to predict:
- Individual agent responses across organizational levels
- Collective organizational adoption outcomes
- Influence propagation through hierarchical authority structures

**Research Question:** *Can deep learning models learn effective representations of organizational dynamics from text to predict responses while respecting game-theoretic properties of hierarchical strategic interaction?*

**Key Documents:**
- [Project Proposal](29102025%20-%20Echo(I)%20-%20Proposal.pdf)
- [Project Roadmap](29102025%20-%20Echo(I)%20-%20Roadmap.pdf)

## ğŸ“ Repository Structure

```
6.S890-Project/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ setup.py                       # Package setup file
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”‚
â”œâ”€â”€ planning/                      # Project planning and documentation
â”‚   â””â”€â”€ project_plan.md           # Detailed project timeline and milestones
â”‚
â”œâ”€â”€ data/                          # Data directory
â”‚   â”œâ”€â”€ raw/                      # Raw, immutable data
â”‚   â”œâ”€â”€ processed/                # Cleaned and preprocessed data
â”‚   â””â”€â”€ interim/                  # Intermediate data transformations
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks for exploration
â”‚   â””â”€â”€ 01_data_exploration.ipynb # Data analysis and visualization
â”‚
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/                     # Data loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py           # Dataset implementations
â”‚   â”‚   â””â”€â”€ dataloader.py        # DataLoader utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                   # Model architectures
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ base_model.py        # Base model class
â”‚   â”‚
â”‚   â”œâ”€â”€ representation/           # Representation learning modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ encoder.py           # Encoder implementations
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                 # Training loops and optimization
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ trainer.py           # Training logic
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/               # Evaluation and metrics
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py           # Metric implementations
â”‚   â”‚
â”‚   â””â”€â”€ utils/                    # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py            # Configuration management
â”‚       â””â”€â”€ logging.py           # Logging utilities
â”‚
â”œâ”€â”€ configs/                      # Configuration files
â”‚   â””â”€â”€ default_config.yaml      # Default hyperparameters
â”‚
â”œâ”€â”€ scripts/                      # Executable scripts
â”‚   â”œâ”€â”€ train.py                 # Training script
â”‚   â””â”€â”€ evaluate.py              # Evaluation script
â”‚
â”œâ”€â”€ experiments/                  # Experiment tracking
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ results/                      # Results and outputs
â”‚   â”œâ”€â”€ checkpoints/             # Model checkpoints
â”‚   â”œâ”€â”€ figures/                 # Generated figures
â”‚   â””â”€â”€ metrics/                 # Evaluation metrics
â”‚
â”œâ”€â”€ tests/                        # Unit tests
â”‚
â””â”€â”€ docs/                         # Additional documentation

```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or higher
- CUDA-capable GPU (recommended)
- 16GB+ RAM

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd 6.S890-Project
```

2. **Create a virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Install the package in development mode**
```bash
pip install -e .
```

### Quick Start

1. **Prepare your data**
   - Place raw data in `data/raw/`
   - Run preprocessing scripts

2. **Configure your experiment**
   - Edit `configs/default_config.yaml`
   - Set hyperparameters and paths

3. **Train a model**
```bash
python scripts/train.py --config configs/default_config.yaml
```

4. **Evaluate the model**
```bash
python scripts/evaluate.py --config configs/default_config.yaml --checkpoint results/checkpoints/best_model.pth
```

## ğŸ”¬ Project Phases

### Phase 1: Planning
- Project definition and scope
- Literature review
- Timeline and milestone planning
- See `planning/project_plan.md` for details

### Phase 2: Data
- **Synthetic Data Generation:** LLM-based generation of 10k organizations with 750k agent responses
- **Real-World Collection:** 500-1000 cases from business schools, corporate reports
- **Agent-Based Modeling:** 100k rule-based simulations for validation
- Exploratory data analysis and quality validation

**Key Notebooks:**
- `notebooks/01_data_exploration.ipynb`: Organizational data analysis
- `notebooks/02_synthetic_generation.ipynb`: LLM-based data generation

**Key Modules:**
- `src/data/org_dataset.py`: Organizational dataset implementations
- `src/data/graph_utils.py`: Authority graph construction
- `scripts/generate_synthetic_data.py`: LLM-based data generation
- `scripts/run_abm_simulation.py`: Agent-based model simulation

### Phase 3: Representation Learning (Dual Encoders)
- **Recommendation Encoder:** Transform-based encoding of recommendation text
- **Organizational Context Encoder:** Encoding of org structure, culture, history
- **Contrastive Learning:** Align semantically compatible recommendation-org pairs
- Embedding quality evaluation (retrieval accuracy, t-SNE visualization)

**Key Modules:**
- `src/representation/dual_encoder.py`: Dual encoder architecture
- `src/representation/contrastive_loss.py`: Contrastive learning objectives
- `src/representation/text_encoder.py`: BERT/RoBERTa encoders

### Phase 4: End-to-End Modeling (H-POSG)
- **Graph Neural Network:** 4-layer GAT for influence propagation
- **Hierarchical Policy Network:** Agent response prediction with hierarchical conditioning
- **Multi-Objective Training:** Prediction + consistency + equilibrium losses
- Integration and end-to-end optimization

**Key Modules:**
- `src/models/h_posg_model.py`: Full H-POSG architecture
- `src/models/gat_propagation.py`: Graph Attention Network
- `src/models/hierarchical_policy.py`: Hierarchical policy network
- `src/training/multi_agent_trainer.py`: Multi-agent training loop
- `src/evaluation/equilibrium_metrics.py`: Game-theoretic evaluation

## ğŸ“Š Experiments

Track experiments using the structure:
```
experiments/
â””â”€â”€ experiment_name/
    â”œâ”€â”€ config.yaml          # Experiment configuration
    â”œâ”€â”€ logs/                # Training logs
    â”œâ”€â”€ checkpoints/         # Model checkpoints
    â””â”€â”€ results.json         # Evaluation results
```

### Recommended Experiment Tracking

We support multiple experiment tracking tools:
- TensorBoard: `tensorboard --logdir results/logs`
- Weights & Biases: Set `use_wandb: true` in config
- MLflow: (Optional) Configure in training script

## ğŸ§ª Testing

Run tests using pytest:
```bash
pytest tests/
```

Run with coverage:
```bash
pytest --cov=src tests/
```

## ğŸ“ˆ Results

Results are organized as:
- `results/checkpoints/`: Saved model checkpoints
- `results/figures/`: Visualization outputs
- `results/metrics/`: Quantitative evaluation results

## ğŸ› ï¸ Development

### Code Style

We follow PEP 8 style guidelines. Format code using:
```bash
black src/
isort src/
```

Check code quality:
```bash
flake8 src/
```

### Adding New Features

1. Create a new branch: `git checkout -b feature/your-feature`
2. Implement your feature in the appropriate module
3. Add tests in `tests/`
4. Update documentation
5. Submit a pull request

## ğŸ“ Configuration

The project uses YAML configuration files. Key configuration sections:

- **data**: Data paths and preprocessing settings
- **model**: Model architecture parameters
- **representation**: Representation learning settings
- **training**: Training hyperparameters
- **evaluation**: Evaluation metrics and settings
- **experiment**: Experiment tracking configuration

See `configs/default_config.yaml` for a complete example.

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ License

[Add your license information]

## ğŸ‘¥ Team

[Add team member information]

## ğŸ™ Acknowledgments

- MIT 6.S890 Course Staff
- [Add other acknowledgments]

## ğŸ“š References

[Add key papers and resources]

## ğŸ“§ Contact

For questions or issues, please [open an issue](link-to-issues) or contact [your-email].

---

**Last Updated:** October 30, 2025
